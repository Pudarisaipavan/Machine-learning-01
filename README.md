# ğŸ’¼ Predicting Income Levels with Stacking Ensemble  

This project demonstrates how to build and train a **stacking ensemble** model using **Logistic Regression**, **Random Forest**, and **XGBoost** to predict income levels from the well-known **Adult dataset**. The project includes data preprocessing, model building, hyperparameter tuning, evaluation, and a set of advanced visualizations to interpret the modelâ€™s performance.  

---

## ğŸ¯ **Objective**  
The goal of this project is to develop a high-performing classification model to predict whether an individual's income exceeds $50K per year based on demographic and employment-related attributes. We aim to:  
âœ… Clean and preprocess the Adult dataset using encoding and scaling  
âœ… Build a **stacking ensemble model** using Logistic Regression, Random Forest, and XGBoost  
âœ… Optimize hyperparameters using **GridSearchCV**  
âœ… Evaluate model performance using ROC AUC, confusion matrix, and classification reports  
âœ… Provide interpretability using SHAP values and other visualization techniques  

---

## ğŸ“‚ **Dataset Overview**  
**Dataset:** Adult Census Income Dataset (UCI Repository)  
- **Samples:** 48,842  
- **Features:** 15 demographic and occupational attributes:  
    - `age` â€“ Age of the individual  
    - `workclass` â€“ Type of employer (e.g., Private, Self-emp, etc.)  
    - `fnlwgt` â€“ Final weight assigned to each row  
    - `education` â€“ Education level (e.g., Bachelors, Masters)  
    - `educational-num` â€“ Numerical version of education level  
    - `marital-status` â€“ Marital status  
    - `occupation` â€“ Job category  
    - `relationship` â€“ Relationship status within the family  
    - `race` â€“ Racial background  
    - `gender` â€“ Male or Female  
    - `capital-gain` â€“ Capital gains  
    - `capital-loss` â€“ Capital losses  
    - `hours-per-week` â€“ Hours worked per week  
    - `native-country` â€“ Country of origin  
    - `income` â€“ **Target**: `<=50K` or `>50K`  

**Target Classes:**  
- `0` â†’ Income <= $50K  
- `1` â†’ Income > $50K  

---

## ğŸ—ï¸ **Model Overview**  
### **Model: Stacking Ensemble**  
The model combines predictions from multiple base learners into a single prediction using a meta-learner:  
âœ… **Base Learners:**  
- Logistic Regression  
- Random Forest  
- XGBoost  

âœ… **Meta-Learner:**  
- Logistic Regression (to aggregate the predictions)  

### **Architecture**  
| Model Type | Hyperparameters | Purpose |
|------------|-----------------|---------|
| **Logistic Regression** | `solver='liblinear'` | Linear model for interpretability |
| **Random Forest** | `n_estimators=100`, `max_depth=None` | Handles non-linear relationships |
| **XGBoost** | `learning_rate=0.1`, `max_depth=5` | Gradient boosting for high accuracy |

**Stacking Strategy:**  
- The base learnersâ€™ predictions are used as input for the meta-learner.  
- The meta-learner learns the best combination of base predictions.  

---

## ğŸ”§ **Hyperparameter Tuning**  
We performed hyperparameter tuning using **GridSearchCV** with the following search space:  

| Parameter | Value Range | Purpose |
|-----------|-------------|---------|
| `final_estimator__C` | [0.1, 1.0, 10.0] | Regularization strength for meta-learner |
| `rf__n_estimators` | [100, 150] | Number of trees in the random forest |
| `xgb__max_depth` | [3, 5] | Tree depth in XGBoost |
| `xgb__learning_rate` | [0.05, 0.1] | Learning rate for XGBoost |

## ğŸ“Š **Performance Metrics**  
On the test set:  

| **Metric** | **Value** |
|-----------|-----------|
| **Accuracy** | ~88% |
| **Precision (<=50K)** | 90% |
| **Recall (<=50K)** | 94% |
| **Precision (>50K)** | 79% |
| **Recall (>50K)** | 66% |
| **F1-Score** | 72% |
| **ROC AUC Score** | 0.928 |

---

### **Confusion Matrix**  
| **Actual** | **Predicted <=50K** | **Predicted >50K** |
|-----------|---------------------|---------------------|
| **<=50K** | 7014 | 417 |
| **>50K** | 795 | 1543 |

---

## ğŸ“ˆ **Results and Visualizations**  
### âœ… **1. Interactive ROC Curve**  
- **AUC = 0.928** â†’ Strong separation capability  
- The curve rises steeply at the beginning â†’ High specificity and sensitivity  

---

### âœ… **2. Stacked Bar Chart (Confusion Matrix)**  
- Displays how actual vs. predicted classes break down.  
- Large blue bar for **<=50K** predictions â†’ High precision for the majority class.  
- Red section for **False Positives** â†’ The model predicts some higher incomes incorrectly.  

---

### âœ… **3. SHAP Beeswarm Plot**  
**SHAP (SHapley Additive exPlanations)** provides insights into feature importance:  
- **Marital-status_Married-civ-spouse** â†’ Strongest positive impact on predicting >50K.  
- **Age** â†’ Older individuals are more likely to have higher incomes.  
- **Educational-num** â†’ Higher education levels increase the probability of earning >50K.  
- **Capital-gain** â†’ High capital gains strongly push the model toward predicting >50K.  

---

### âœ… **4. Gaussian-Smoothed Learning Curve**  
- Demonstrates consistent model improvement during training.  
- Smooth decline in loss â†’ No signs of overfitting.  

1. **Clone the Repository:**
 ```bash
 git clone https://github.com/Pudarisaipavan/income-classification.git
